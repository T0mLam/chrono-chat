# ChronoChat

**ChronoChat** is a UI for **Ollama** that enables users to chat with video content using non vision/video-language models (VLMs). It supports both YouTube and local uploads and uses retrieval-augmented generation (RAG) to answer questions using video transcripts, frames, and captions. Powered by local LLMs, ChronoChat streams real-time responses with additional support for images and PDF uploads.


https://github.com/user-attachments/assets/983ef2d6-f9cb-410c-8d3a-13bcc2a35c0e


> [!NOTE]
> **ChronoChat is ideal for:** </br>
> - âœ… Interviews, tutorials, and educational content </br>
> - âŒ Not suited for animations or silent videos </br>
>
> **âš ï¸ Requires a GPU for optimal performance**

## ğŸ Getting Started

### 1. ğŸ“¦ Set Up Python Environment

```bash
# Create and activate a virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### 2. ğŸ”¨ Install Dependencies for ChronoChat

```bash
python cli.py install
```

### 3. âš™ï¸ Install PyTorch with CUDA (Recommended)

For GPU acceleration, install the CUDA-enabled version of PyTorch: <br />
Visit https://pytorch.org/get-started/locally/ to get the correct command for your system.

> ğŸ’¡ If you donâ€™t have an NVIDIA GPU or donâ€™t want CUDA, skip this step

### 4. ğŸï¸ Install FFmpeg

ChronoChat requires `ffmpeg` for processing video and audio. <br />
Download from: [https://ffmpeg.org/download.html](https://ffmpeg.org/download.html)

### 5. ğŸ¤– Install Ollama

If you havenâ€™t already, install [Ollama](https://ollama.com)

### 6. ğŸ–¥ï¸ Start the Ollama Server

```bash
ollama serve
```

### 7. ğŸš€ Launch ChronoChat

```bash
python cli.py start
```

Then open your browser at: [http://localhost:3000](http://localhost:3000)

## âœ¨ Key Features

* ğŸ” **Video RAG**: Uses CLIP, Whisper, and BLIP embeddings for frame, audio, and caption-based retrieval.
* ğŸ§  **LLM Planning**: Models generate reasoning chains, plan actions, and adapt to single or multi-video chats.
* ğŸ”Œ **Streaming Responses**: Live WebSocket chat with markdown rendering and response progress updates.
* ğŸ¥ **Multi-Video Support**: Search and reason across multiple videos in a single conversation.
* ğŸ“ **Attach Files**: Supports uploading PDFs and images.

## ğŸ§± Architecture

```mermaid
---
config:
  look: handDrawn
  theme: neutral
---

graph TD
  subgraph "Frontend (Next.js)"
    Sidebar["ğŸ“‚ Chats & Videos"]
    UploadUI["ğŸ“¦ Upload videos"]
    ChatUI["ğŸ’¬ Chat interface"]
    APIClient["ğŸ”— REST client"]
    WSClient["ğŸ”„ WebSocket client"]
  end

  subgraph "Backend (FastAPI & Async Worker)"
    ChatRouter["ğŸ—¨ï¸ Chat router"]
    MediaRouter["ğŸ¬ Media router"]
    VideoRAG["ğŸ§  VideoRAG engine"]
    ContextExtractor["ğŸ” Context extractor"]
    Retriever["ğŸ“¦ ChromaDB retriever"]
    LLMClient["ğŸ¤– LLM client"]
    Worker["âš™ï¸ Ingestion worker"]
    MediaDB["ğŸ—„ï¸ ChromaDB"]
    MediaStorage["ğŸ“ Video and metadata storage"]
    VideoQueue["ğŸ“® Processing queue"]
  end

  Sidebar --> ChatUI
  UploadUI --> APIClient
  ChatUI -- "File upload" --> APIClient
  ChatUI <--"Text query" --> WSClient

  APIClient <--> MediaRouter
  WSClient <--> ChatRouter
  ChatRouter --> VideoRAG
  VideoRAG <-- "Video query" --> ContextExtractor
  VideoRAG <-- "Other query" --> LLMClient
  ContextExtractor <--> Retriever
  ContextExtractor <--> LLMClient
  Retriever <--> MediaDB

  MediaRouter --> MediaStorage
  MediaRouter --> VideoQueue
  VideoQueue --> Worker
  Worker --> MediaDB
```

## âš™ï¸ Tech Stack

| Layer      | Tools                                 |
| ---------- | ------------------------------------- |
| Frontend   | Next.js, TailwindCSS, Shadcn, TypeScript |
| Backend    | FastAPI, AsyncIO, SQLite, ChromaDB     |
| Embeddings | CLIP (frames), Whisper (audio), BLIP  |
| LLM        | Ollama       |
| Storage    | Local files, ChromaDB vectors, SQLite |

## ğŸ§  How It Works

1. **Ingest Video**: Extracts audio, frames, and captions from YouTube/local videos.
2. **Embed Content**: Computes multimodal embeddings and stores them in ChromaDB.
3. **Chat Interaction**: LLM receives the user query and selects a retrieval mode.
4. **RAG Flow**: Relevant chunks are retrieved based on video context.
5. **Response Streaming**: Final output is streamed to the user in real time.
